{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8022663,"sourceType":"datasetVersion","datasetId":4727630}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-10T12:16:13.976824Z","iopub.execute_input":"2025-09-10T12:16:13.977448Z","iopub.status.idle":"2025-09-10T12:16:14.238204Z","shell.execute_reply.started":"2025-09-10T12:16:13.977423Z","shell.execute_reply":"2025-09-10T12:16:14.237450Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/liar-dataset/test.tsv\n/kaggle/input/liar-dataset/README\n/kaggle/input/liar-dataset/train.tsv\n/kaggle/input/liar-dataset/valid.tsv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from torch.optim import AdamW","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:14:31.124678Z","iopub.execute_input":"2025-09-10T14:14:31.125017Z","iopub.status.idle":"2025-09-10T14:14:31.128906Z","shell.execute_reply.started":"2025-09-10T14:14:31.124993Z","shell.execute_reply":"2025-09-10T14:14:31.128338Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport re\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nfrom transformers import RobertaTokenizerFast, RobertaForSequenceClassification, get_scheduler\nfrom torch.optim import Adam\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:14:47.984667Z","iopub.execute_input":"2025-09-10T14:14:47.985250Z","iopub.status.idle":"2025-09-10T14:14:47.989462Z","shell.execute_reply.started":"2025-09-10T14:14:47.985227Z","shell.execute_reply":"2025-09-10T14:14:47.988752Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\ntrain_df = pd.read_csv(\"/kaggle/input/liar-dataset/train.tsv\", sep=\"\\t\", header=None)\nval_df   = pd.read_csv(\"/kaggle/input/liar-dataset/valid.tsv\", sep=\"\\t\", header=None)\ntest_df  = pd.read_csv(\"/kaggle/input/liar-dataset/test.tsv\", sep=\"\\t\", header=None)\n\ncols = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job_title\", \"state\", \"party\",\n        \"barely_true_ct\", \"false_ct\", \"half_true_ct\", \"mostly_true_ct\", \"pants_fire_ct\",\n        \"context\"]\ntrain_df.columns = cols\nval_df.columns = cols\ntest_df.columns = cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:30:24.116580Z","iopub.execute_input":"2025-09-10T14:30:24.117196Z","iopub.status.idle":"2025-09-10T14:30:24.193695Z","shell.execute_reply.started":"2025-09-10T14:30:24.117176Z","shell.execute_reply":"2025-09-10T14:30:24.192937Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"label_map = {\n    \"pants-fire\": 0,\n    \"false\": 0,\n    \"barely-true\": 0,\n    \"half-true\": 0,\n    \"mostly-true\": 1,\n    \"true\": 1\n}\nfor df in [train_df, val_df, test_df]:\n    df[\"label\"] = df[\"label\"].map(label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:30:45.124982Z","iopub.execute_input":"2025-09-10T14:30:45.125265Z","iopub.status.idle":"2025-09-10T14:30:45.135380Z","shell.execute_reply.started":"2025-09-10T14:30:45.125246Z","shell.execute_reply":"2025-09-10T14:30:45.134653Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def clean_text(text):\n    text = str(text).lower()\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\nfor df in [train_df, val_df, test_df]:\n    df[\"statement\"] = df[\"statement\"].apply(clean_text)\n    df[\"context\"]   = df[\"context\"].fillna(\"\").apply(clean_text)\n    df[\"speaker\"]   = df[\"speaker\"].fillna(\"\").apply(clean_text)\n    df[\"party\"]     = df[\"party\"].fillna(\"\").apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:31:02.892648Z","iopub.execute_input":"2025-09-10T14:31:02.892898Z","iopub.status.idle":"2025-09-10T14:31:03.162757Z","shell.execute_reply.started":"2025-09-10T14:31:02.892882Z","shell.execute_reply":"2025-09-10T14:31:03.161962Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def combine_text(row):\n    return (\n        row[\"statement\"] + \" [SEP] speaker: \" + row[\"speaker\"] +\n        \" party: \" + row[\"party\"] + \" context: \" + row[\"context\"]\n    )\n\ntrain_texts = train_df.apply(combine_text, axis=1).tolist()\nval_texts   = val_df.apply(combine_text, axis=1).tolist()\ntest_texts  = test_df.apply(combine_text, axis=1).tolist()\n\ntrain_labels = train_df[\"label\"].tolist()\nval_labels   = val_df[\"label\"].tolist()\ntest_labels  = test_df[\"label\"].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:31:22.230448Z","iopub.execute_input":"2025-09-10T14:31:22.231161Z","iopub.status.idle":"2025-09-10T14:31:22.360217Z","shell.execute_reply.started":"2025-09-10T14:31:22.231136Z","shell.execute_reply":"2025-09-10T14:31:22.359322Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"class LIARDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = int(self.labels[idx])\n\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n            \"labels\": torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:31:42.565998Z","iopub.execute_input":"2025-09-10T14:31:42.566261Z","iopub.status.idle":"2025-09-10T14:31:42.571931Z","shell.execute_reply.started":"2025-09-10T14:31:42.566245Z","shell.execute_reply":"2025-09-10T14:31:42.571148Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n\ntrain_dataset = LIARDataset(train_texts, train_labels, tokenizer)\nval_dataset   = LIARDataset(val_texts, val_labels, tokenizer)\ntest_dataset  = LIARDataset(test_texts, test_labels, tokenizer)\n\nclass_counts = train_df[\"label\"].value_counts().to_dict()\nclass_weights = [1.0 / class_counts[label] for label in train_df[\"label\"]]\nsampler = WeightedRandomSampler(class_weights, num_samples=len(class_weights), replacement=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\nval_loader   = DataLoader(val_dataset, batch_size=16)\ntest_loader  = DataLoader(test_dataset, batch_size=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:32:04.958476Z","iopub.execute_input":"2025-09-10T14:32:04.958763Z","iopub.status.idle":"2025-09-10T14:32:05.764740Z","shell.execute_reply.started":"2025-09-10T14:32:04.958743Z","shell.execute_reply":"2025-09-10T14:32:05.763960Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\nmodel.to(device)\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=2e-5)\nnum_training_steps = len(train_loader) * 6  # 6 epochs\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n                             num_warmup_steps=0, num_training_steps=num_training_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:32:46.406360Z","iopub.execute_input":"2025-09-10T14:32:46.407069Z","iopub.status.idle":"2025-09-10T14:32:47.073462Z","shell.execute_reply.started":"2025-09-10T14:32:46.407046Z","shell.execute_reply":"2025-09-10T14:32:47.072867Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, scheduler, loss_fn, epochs=6):\n    best_f1, patience, patience_counter = 0, 2, 0\n\n    for epoch in range(epochs):\n        model.train()\n        loop = tqdm(train_loader, leave=True)\n        total_loss, total_acc = 0, 0\n\n        for batch in loop:\n            optimizer.zero_grad()\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n\n            preds = torch.argmax(logits, dim=1)\n            acc = (preds == labels).cpu().numpy().mean()\n\n            total_loss += loss.item()\n            total_acc += acc\n\n            loop.set_description(f\"Epoch {epoch+1}\")\n            loop.set_postfix(loss=loss.item(), acc=acc)\n\n        avg_loss = total_loss / len(train_loader)\n        avg_acc = total_acc / len(train_loader)\n        print(f\"\\nEpoch {epoch+1} | Train Loss: {avg_loss:.4f} | Train Acc: {avg_acc:.4f}\")\n\n        val_acc, val_f1 = evaluate(model, val_loader, split=\"Validation\")\n\n        # Early stopping\n        if val_f1 > best_f1:\n            torch.save(model.state_dict(), \"best_roberta.pt\")\n            best_f1 = val_f1\n            patience_counter = 0\n            print(\" Saved new best model\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\" Early stopping triggered\")\n                break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:33:17.585411Z","iopub.execute_input":"2025-09-10T14:33:17.585934Z","iopub.status.idle":"2025-09-10T14:33:17.593378Z","shell.execute_reply.started":"2025-09-10T14:33:17.585900Z","shell.execute_reply":"2025-09-10T14:33:17.592668Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"def evaluate(model, loader, split=\"Test\"):\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for batch in loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            label = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            pred = torch.argmax(logits, dim=1)\n\n            preds.extend(pred.cpu().numpy())\n            labels.extend(label.cpu().numpy())\n\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    print(f\"{split} Accuracy: {acc:.4f}, {split} F1: {f1:.4f}\")\n    if split != \"Validation\":\n        print(classification_report(labels, preds))\n        print(\"Confusion Matrix:\\n\", confusion_matrix(labels, preds))\n    return acc, f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:33:52.685437Z","iopub.execute_input":"2025-09-10T14:33:52.686199Z","iopub.status.idle":"2025-09-10T14:33:52.692173Z","shell.execute_reply.started":"2025-09-10T14:33:52.686175Z","shell.execute_reply":"2025-09-10T14:33:52.691371Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"train_model(model, train_loader, val_loader, optimizer, lr_scheduler, loss_fn, epochs=6)\n\n\nmodel.load_state_dict(torch.load(\"best_roberta.pt\"))\nevaluate(model, test_loader, split=\"Final Test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:34:10.155035Z","iopub.execute_input":"2025-09-10T14:34:10.155698Z","iopub.status.idle":"2025-09-10T14:43:40.176343Z","shell.execute_reply.started":"2025-09-10T14:34:10.155675Z","shell.execute_reply":"2025-09-10T14:43:40.175564Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 640/640 [02:15<00:00,  4.73it/s, acc=0.688, loss=0.611]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 | Train Loss: 0.6705 | Train Acc: 0.5920\nValidation Accuracy: 0.6659, Validation F1: 0.6638\n Saved new best model\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 640/640 [02:16<00:00,  4.68it/s, acc=0.938, loss=0.262]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 | Train Loss: 0.5820 | Train Acc: 0.7074\nValidation Accuracy: 0.6643, Validation F1: 0.6654\n Saved new best model\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 640/640 [02:16<00:00,  4.71it/s, acc=0.688, loss=0.575]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 | Train Loss: 0.4965 | Train Acc: 0.7733\nValidation Accuracy: 0.6760, Validation F1: 0.6652\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 640/640 [02:16<00:00,  4.68it/s, acc=0.688, loss=0.652]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 | Train Loss: 0.4112 | Train Acc: 0.8254\nValidation Accuracy: 0.6526, Validation F1: 0.6616\n Early stopping triggered\nFinal Test Accuracy: 0.6661, Final Test F1: 0.6666\n              precision    recall  f1-score   support\n\n           0       0.74      0.74      0.74       818\n           1       0.53      0.53      0.53       449\n\n    accuracy                           0.67      1267\n   macro avg       0.64      0.64      0.64      1267\nweighted avg       0.67      0.67      0.67      1267\n\nConfusion Matrix:\n [[604 214]\n [209 240]]\n","output_type":"stream"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"(0.6661404893449092, 0.6665530553398538)"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"import torch\nimport shutil\n\n\nsave_dir = \"/kaggle/working/roberta_liar_model\"\nmodel.save_pretrained(save_dir)\ntokenizer.save_pretrained(save_dir)\n\nextra_info = {\n    \"task\": \"Fake News Detection - LIAR dataset (binary)\",\n    \"model\": \"roberta-base\",\n    \"final_test_accuracy\": float(test_acc),\n    \"final_test_f1\": float(test_f1)\n}\n\nimport json\nwith open(save_dir + \"/meta.json\", \"w\") as f:\n    json.dump(extra_info, f, indent=4)\n\n# 3. Zip the folder\nshutil.make_archive(\"/kaggle/working/roberta_liar_model\", 'zip', save_dir)\n\nprint(\"✅ Model + tokenizer saved and zipped in /kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:56:43.556714Z","iopub.execute_input":"2025-09-10T14:56:43.557036Z","iopub.status.idle":"2025-09-10T14:57:26.220982Z","shell.execute_reply.started":"2025-09-10T14:56:43.557017Z","shell.execute_reply":"2025-09-10T14:57:26.220318Z"}},"outputs":[{"name":"stdout","text":"✅ Model + tokenizer saved and zipped in /kaggle/working/\n","output_type":"stream"}],"execution_count":59}]}